---
date: 2019-09-08 10:44:00
layout: post
title: Boosting
subtitle: boosting method
description: boosting method 
image: https://github.com/birkhoffkiki/Personal-Website/blob/master/assets/img/post_imgs/2019-09-16-01-cat.jpg?raw=true

category: blog
tags:
  - boosting
  - ensemble
  - blog
author: majiabo
paginate: true
---

# Boosting 调研  

在我当前研究工作中，需要有效的整合多个模型，因为没有合适的方法整合。所以想要系统性的了解集成学习的相关知识和方法，因此该读书笔记将记录boosting方法的历史发展和经典方法。主要包括以下几个方面：  

1. [集成学习的简要介绍](#id0)  
2. [boosting方法概述](#id1)  
3. [Adaboost](#id2)  
4. [GBDT](#id3)  
5. [XGBoost](#id4)  

 <span id="id0"></span>  

## 1. 集成学习的简要介绍  

### 1.1 什么是集成学习[^1]

[^1]:参考《机器学习》，周志华，清华大学出版社  
> 集成学习是通过构建并合并多个分类器来完成学习任务，有时也被称为多分类器系统(multi-classifier system)、基于委员会的学习(committee-based learning)等。下图显示了集成学习的一般结构：  

```mermaid
graph TD
A(学习器 1) -->Z(整合模块)  
B(学习器 2) -->Z(整合模块)  
C(学习器 3) -->Z(整合模块)  
D(学习器 n) --> Z  
    Z-->R(输出)
```

在集成学习中，先产生一组“个体学习器”(individual learner), 再用某种策略将它们结合起来。个体学习器通常有现有的学习算法从训练数据产生，例如决策树算法，BP神经网络网络等。若个体学习器都是相同的，比如所有的个体学习器都是“决策树”或者“BP神经网络”，则称这样的集成是“同质”的(homogeneous)。同质集成中的个体学习器也称“基学习器”（base learner), 相应的学习算法称之为“基学习算法”(base learning algorithm)  
当然集成的学习器中也可以包含不同种类的个体学习器，比如同时包含“决策树”和“BP神经网络”，这样的集成称之为“异质”的(heterogenous)。由于异质学习器的个体学习器不同，所以个体学习器由不同的学习算法生成，此时就没有统一的基学习算法。相应的，个体学习器不再称之为基学习器而是被称之为“组件学习器”(component learner)或直接称之为个体学习器。  
集成学习器通过将多个学习器进行结合，通常可以获得比单一学习器显著优越的泛化性能。集成尤其对“弱学习器”(weak learner)尤为明显，因此很多集成学习的研究都是针对弱学习器进行的，于是有时基学习器也被称做弱学习器。虽然从理论上来说，集成足够的弱学习器足以获得很好的性能，但在实践中考虑到使用的学习器的个数等种种因素，人们往往会使用较强的学习器。  

### 1.2 集成学习的分类  
按照集成目的分类，一般可以划分为Bagging(bootstrap aggregation),boosting以及stacking。
> *Bagging* 对数据集进行重采样，每个数据点被等概率采样，然后创建n个模型。其中每个模型均由m个采样数据得到,最终采用投票的方式得到最后的结果。  

* Bagging能够降低variance，改善模型的泛化误差。
* 性能依赖于基分类器的稳定性；若基分类器不稳定，则bagging有助于降低随机误差；若个体分类器稳定，则集成分类器的误差主要由基分类器的偏差引起。
* 由于每个样本被选中的概率相同，所以bagging不侧重于训练数据集中的任何实例。

> **boosting** 是一族可以将弱学习器提升为强学习器的算法。这种方法首先在初始训练集上训练得到一个基分类器，然后根据基分类器的表现调整数据分布，使得先前在基分类器上表现效果比较差的样本在后续受到更多的关注。再在调整分布后的数据上训练新的基分类器，不断迭代直至达到预先设定的基分类器的个数。可以用下列流程图示意这个过程：  

```mermaid
graph LR
data_0(原始数据) --train--> cls_0(基模型0)  
data_0(原始数据) --> data_1(调整分布数据1)  
cls_0(基模型0) --adjust-->data_1(调整分布数据1)

data_1(调整分布数据1) --train--> cls_1(基模型1)  
data_1(调整分布数据1) --> data_2(调整分布数据2)  
cls_1(基模型1)--adjust-->data_2(调整分布数据2)

data_2(调整分布数据2) --train--> cls_2(...)  
data_2(调整分布数据2) --> data_3(...)  
cls_2(...)--adjust-->data_3(...)

data_3(...) --train--> cls_n(基模型n)  
data_3(...) --> datan(调整数据分布n+1)
cls_n(基模型n) --adjust-->datan(调整数据分布n+1)

cls_0(基模型0) --> final(综合)
cls_1(基模型1) --> final(综合)
cls_2(...) --> final(综合)
cls_n(基模型n) --> final(综合)
final(综合) --> result((结果))

```

* boosting主要关注降低偏差，因此其可以基于泛化性能相当弱的分类器构建出很强的集成模型。

* **每一轮训练过程中如何改变数据的权值或概率分布？**

* **通过什么方式组合弱分类器？**

> **stacking** 先从初始数据集中训练出初级学习器，然后“生成”一个新数据集用于训练次级学习器。在训练次级学习器的过程中，将初级学习器的输出作为次级学习器的输入(例如：n个初级学习器的预测结果所组成的向量作为次级学习器的输入)而依旧将初始数据集的标签作为次级学习器的标签。初级学习器可以是异质的，意味着初级学习器可以使用各种各样的模型实现。

<span id="id1"></span>  

## 2. boosting方法概述[^2]  

[^2]:https://en.wikipedia.org/wiki/Boosting_(machine_learning)


<span id="id2"></span>

## 3. Adaboost  


<span id="id3"></span>

## 4. GBDT  


<span id="id4"></span>

## 5. XGBoost
